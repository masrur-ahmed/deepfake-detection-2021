{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n# https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch\n# Install facenet-pytorch\n!pip install /kaggle/input/facenet-pytorch-vggface2/facenet_pytorch-1.0.1-py3-none-any.whl\n# Copy model checkpoints to torch cache so they are loaded automatically by the package\n!mkdir -p /tmp/.cache/torch/checkpoints/\n!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-logits.pth /tmp/.cache/torch/checkpoints/vggface2_DG3kwML46X.pt\n!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-features.pth /tmp/.cache/torch/checkpoints/vggface2_G5aNV2VSMn.pt","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport gc\nimport cv2\nimport glob\nimport time\nimport copy\nfrom tqdm import tqdm_notebook as tqdm\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import models, transforms\nfrom facenet_pytorch import MTCNN, InceptionResnetV1","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\npackage_path = '../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master'\nsys.path.append(package_path)\n\nfrom efficientnet_pytorch import EfficientNet","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(0)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set Trained Weight Path\nweight_path = 'efficientnet_b0_epoch_15_loss_0.158.pth'\ntrained_weights_path = os.path.join('../input/deepfake-detection-model-weight', weight_path)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark=True","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dir = '../input/deepfake-detection-challenge/test_videos'\nos.listdir(test_dir)[:5]","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"['ywauoonmlr.mp4',\n 'sfsayjgzrh.mp4',\n 'sngjsueuhs.mp4',\n 'eryjktdexi.mp4',\n 'nwvloufjty.mp4']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_img_from_mov(video_file, num_img, frame_window):\n    # https://note.nkmk.me/python-opencv-videocapture-file-camera/\n    cap = cv2.VideoCapture(video_file)\n    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    image_list = []\n    for i in range(num_img):\n        _, image = cap.read()\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_list.append(image)\n        cap.set(cv2.CAP_PROP_POS_FRAMES, (i + 1) * frame_window)\n        if cap.get(cv2.CAP_PROP_POS_FRAMES) >= frames:\n            break\n    cap.release()\n\n    return image_list","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageTransform:\n    def __init__(self, size, mean, std):\n        self.data_transform = transforms.Compose([\n                transforms.Resize((size, size), interpolation=Image.BILINEAR),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std)\n            ])\n\n    def __call__(self, img):\n        return self.data_transform(img)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DeepfakeDataset(Dataset):\n    def __init__(self, file_list, device, detector, transform, img_num=20, frame_window=10):\n        self.file_list = file_list\n        self.device = device\n        self.detector = detector\n        self.transform = transform\n        self.img_num = img_num\n        self.frame_window = frame_window\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n\n        mov_path = self.file_list[idx]\n        img_list = []\n\n        # Movie to Image\n        try:\n            all_image = get_img_from_mov(mov_path, self.img_num, self.frame_window)\n        except:\n            return [], mov_path.split('/')[-1]\n        \n        # Detect Faces\n        for image in all_image:\n            \n            try:\n                _image = image[np.newaxis, :, :, :]\n                boxes, probs = self.detector.detect(_image, landmarks=False)\n                x = int(boxes[0][0][0])\n                y = int(boxes[0][0][1])\n                z = int(boxes[0][0][2])\n                w = int(boxes[0][0][3])\n                image = image[y-15:w+15, x-15:z+15]\n                \n                # Preprocessing\n                image = Image.fromarray(image)\n                image = self.transform(image)\n                \n                img_list.append(image)\n\n            except:\n                img_list.append(None)\n            \n        # Padding None\n        img_list = [c for c in img_list if c is not None]\n        \n        return img_list, mov_path.split('/')[-1]","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = EfficientNet.from_name('efficientnet-b0')\nmodel._fc = nn.Linear(in_features=model._fc.in_features, out_features=1)\nmodel.load_state_dict(torch.load(trained_weights_path, map_location=torch.device(device)))","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_file = [os.path.join(test_dir, path) for path in os.listdir(test_dir)]","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_file[:5]","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"['../input/deepfake-detection-challenge/test_videos/ywauoonmlr.mp4',\n '../input/deepfake-detection-challenge/test_videos/sfsayjgzrh.mp4',\n '../input/deepfake-detection-challenge/test_videos/sngjsueuhs.mp4',\n '../input/deepfake-detection-challenge/test_videos/eryjktdexi.mp4',\n '../input/deepfake-detection-challenge/test_videos/nwvloufjty.mp4']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\ndef predict_dfdc(dataset, model):\n    \n    torch.cuda.empty_cache()\n    pred_list = []\n    path_list = []\n    \n    model = model.to(device)\n    model.eval()\n\n    with torch.no_grad():\n        for i in tqdm(range(len(dataset))):\n            pred = 0\n            imgs, mov_path = dataset.__getitem__(i)\n            \n            # No get Image\n            if len(imgs) == 0:\n                pred_list.append(0.5)\n                path_list.append(mov_path)\n                continue\n                \n                \n            for i in range(len(imgs)):\n                img = imgs[i]\n                \n                output = model(img.unsqueeze(0).to(device))\n                pred += torch.sigmoid(output).item() / len(imgs)\n                \n            pred_list.append(pred)\n            path_list.append(mov_path)\n            \n    torch.cuda.empty_cache()\n            \n    return path_list, pred_list","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Config\nimg_size = 120\nimg_num = 15\nframe_window = 5\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\n\ntransform = ImageTransform(img_size, mean, std)\n\ndetector = MTCNN(image_size=img_size, margin=14, keep_all=False, factor=0.5, \n                 select_largest=False, post_process=False, device=device).eval()\n\ndataset = DeepfakeDataset(test_file, device, detector, transform, img_num, frame_window)\n\npath_list, pred_list = predict_dfdc(dataset, model)","execution_count":15,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  if sys.path[0] == '':\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=400), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5297e280f07461681a936778d0883b7"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission\nres = pd.DataFrame({\n    'filename': path_list,\n    'label': pred_list,\n})\n\nres.sort_values(by='filename', ascending=True, inplace=True)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(res['label'], 20)\nplt.show()","execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"res.head(10)","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"           filename     label\n374  aassnaulhq.mp4  0.980907\n247  aayfryxljh.mp4  0.000571\n339  acazlolrpz.mp4  0.863356\n193  adohdulfwb.mp4  0.027974\n315  ahjnxtiamx.mp4  0.713677\n248  ajiyrjfyzp.mp4  0.597599\n367  aktnlyqpah.mp4  0.999882\n340  alrtntfxtd.mp4  0.892700\n292  aomqqjipcp.mp4  0.996379\n348  apedduehoy.mp4  0.022171","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>374</th>\n      <td>aassnaulhq.mp4</td>\n      <td>0.980907</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>aayfryxljh.mp4</td>\n      <td>0.000571</td>\n    </tr>\n    <tr>\n      <th>339</th>\n      <td>acazlolrpz.mp4</td>\n      <td>0.863356</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>adohdulfwb.mp4</td>\n      <td>0.027974</td>\n    </tr>\n    <tr>\n      <th>315</th>\n      <td>ahjnxtiamx.mp4</td>\n      <td>0.713677</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>ajiyrjfyzp.mp4</td>\n      <td>0.597599</td>\n    </tr>\n    <tr>\n      <th>367</th>\n      <td>aktnlyqpah.mp4</td>\n      <td>0.999882</td>\n    </tr>\n    <tr>\n      <th>340</th>\n      <td>alrtntfxtd.mp4</td>\n      <td>0.892700</td>\n    </tr>\n    <tr>\n      <th>292</th>\n      <td>aomqqjipcp.mp4</td>\n      <td>0.996379</td>\n    </tr>\n    <tr>\n      <th>348</th>\n      <td>apedduehoy.mp4</td>\n      <td>0.022171</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}